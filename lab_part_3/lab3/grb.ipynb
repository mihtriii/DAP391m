{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Machine Learning 2: predictive models, deep learning, neural network 2022Z\n",
    "\n",
    "# Gradient Boosting (GRB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "- AdaBoost was the first successful boosting algorithm [Freund et al., 1996, Freund and Schapire, 1997]\n",
    "- the most successful form of the AdaBoost algorithm for binary classification problems is called AdaBoost.M1\n",
    "- belongs to the group of algorithms called Arcing - Adaptive Reweighting and Combining algorithms\n",
    "  - a weighted minimization followed by a recomputation of the classifiers and weighted input\n",
    "  \n",
    "<img src=\"Schapire.png\">\n",
    "\n",
    "#### Main idea\n",
    "- model is a combination of many \"weak learners\" - decision trees with a single split <B>(stumps)</B>\n",
    "\n",
    "#### Key features\n",
    "- puts more weight on difficult to classify instances and less on those already handled well\n",
    "- order of the stumps is important,error of the first stump influence on how the second stump is made\n",
    "- some stumps have the greater weights(votes) than the others\n",
    "- final classification is made by total number of votes for given output \n",
    "\n",
    "## Example AdaBoost\n",
    "\n",
    "### Data set\n",
    "\n",
    "<img src=\"adaBoost1.png\">\n",
    "\n",
    "### Stumps\n",
    "<img src=\"adaBoost2.png\">\n",
    "\n",
    "### Algorithm\n",
    "<img src=\"adaBoost0.png\">\n",
    "\n",
    "<img src=\"adaBoost3.png\">\n",
    "\n",
    "<img src=\"adaBoost4.png\">\n",
    "\n",
    "<img src=\"adaBoost5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "- ML technique for both regression and classification problems\n",
    "- develops a strong learner by combining weak learners in iterative way\n",
    "\n",
    "#### History:\n",
    "- Formulate Adaboost as gradient descent with a special loss function[Breiman et al., 1998, Breiman, 1999]\n",
    "- Generalize Adaboost to Gradient Boosting in order to handle a variety of loss functions [Friedman et al., 2000, Friedman, 2001]\n",
    "\n",
    "#### Friedman\n",
    "<img src=\"Friedman.png\">\n",
    "\n",
    "#### Main idea\n",
    "- minimize the loss of the model by adding weak learners using a gradient descent like procedure\n",
    "\n",
    "#### Why GRB is stage-wise additive model?\n",
    "- one new weak learner is added at a time\n",
    "- existing weak learners in the model are frozen and left unchanged\n",
    "\n",
    "### Gradient boosting involves three elements\n",
    "1. A loss function to be optimized.\n",
    "2. A weak learner to make predictions.\n",
    "3. An additive model to add weak learners to minimize the loss function.\n",
    "\n",
    "#### Loss Function\n",
    "- depends on the type of problem being solved\n",
    "- must be differentiable\n",
    "- regression may use a squared error and classification may use logarithmic loss\n",
    "\n",
    "#### Weak Learner\n",
    "- subsequent regression trees are used to correct the residuals in the predictions\n",
    "- regression trees have usually with 4-to-8 levels\n",
    "- possible parameters are  maximum number of layers,nodes, splits or leaf nodes\n",
    "- trees are constructed in a greedy manner (best split points based on purity scores like Gini)\n",
    "\n",
    "#### Additive Model\n",
    "- trees are added one at a time, \n",
    "- existing trees in the model are not changed\n",
    "- a gradient descent procedure is used to minimize the loss when adding trees (reducing the residual loss)\n",
    "\n",
    "<img src=\"grbDiagram1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example  Gradient Boosting - Regression \n",
    "<img src=\"grbXLS1.png\">\n",
    "\n",
    "## 1st tree\n",
    "    a) Predicted_weight=Avg(Weights)=71,2\n",
    "    b) Move the residuals into the leafs\n",
    "    c) Calculate average residual for each leaf\n",
    "\n",
    "<img src=\"grbXLS2.png\">\n",
    "<img src=\"grbDiagram2a.png\">\n",
    "\n",
    "## 2nd tree\n",
    "\n",
    "    a) Predicted_weight=Avg(Weights)+learningRate*1st_Tree_AvgRes\n",
    "    b) Move the residuals into the leafs\n",
    "    c) Calculate average residual for each leaf\n",
    "\n",
    "<img src=\"grbXLS3.png\">\n",
    "<img src=\"grbDiagram2b.png\">\n",
    "\n",
    "## 3rd tree\n",
    "\n",
    "    a) Predicted_weight=Avg(Weights)+learningRate (1st_Tree_AvgRes+2nd_Tree_AvgRes)\n",
    "    b) Move the residuals into the leafs\n",
    "    c) Calculate average residual for each leaf\n",
    "\n",
    " <img src=\"grbXLS4.png\"> \n",
    " \n",
    "# Example  Gradient Boosting - Classification \n",
    " <img src=\"grbXLS5.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing hyper-parameters:\n",
    "- the number of stages M , higher number increases the accuracy on the training set,but ....\n",
    "- the learning rate (also known as shrinkage)\n",
    "\n",
    "\n",
    "#### Remark\n",
    "Tuning the hyper-parameters is required to get a decent GBM model unlike Random Forests.\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "- GRB as a greedy algorithm can overfit a training dataset quickly\n",
    "\n",
    "\n",
    "### Improvements to Basic Gradient Boosting\n",
    "- Tree Constraints\n",
    "- Shrinkage\n",
    "- Random Sampling.\n",
    "- Penalized Learning\n",
    "\n",
    "#### Tree Constraints\n",
    "- weak learners have skill but remain weak\n",
    "- the more constrained tree creation is, the more trees you will need in the model\n",
    "- shorter trees are preferred.\n",
    "- constraints: maximum number of layers,nodes, splits or leaf nodes\n",
    "\n",
    "#### Shrinkage (learning rate)\n",
    "- the contribution of each tree to the total sum\n",
    "- slows down  the learning by the algorithm\n",
    "- use small values <0.1-0.3>\n",
    "\n",
    "#### Stochastic Gradient Boosting\n",
    "- allows trees to be greedily created from subsamples of the training dataset\n",
    "- reduce the correlation between the trees in the sequence in gradient boosting models\n",
    "- at each iteration a subsample of the training data is drawn at random (without replacement) from the full training dataset\n",
    "\n",
    "#### Penalized Gradient Boosting\n",
    "- use a regression trees (numeric values in the leaf nodes)\n",
    "- values in the leaves of the trees - can be called weights and regularized using popular regularization functions:\n",
    "  - L1 regularization of weights\n",
    "  <img src=\"L1.png\">\n",
    "  - L2 regularization of weights\n",
    "  <img src=\"L2.png\">\n",
    "\n",
    "## Extreme gradient boosting (XGBoost)\n",
    "- refined and customized version of a gradient boosting decision tree system\n",
    "\n",
    "\n",
    "### What is XGBoost?\n",
    "- one of the most popular implementations of the Gradient Boosted Trees algorithms, created by Tianqi Chen,\n",
    "- a broader collection of tools under the umbrella of the Distributed Machine Learning Community creators of mxnet deep learning library-\n",
    "- regularized form of Gradient Boosting\n",
    "- fast compared to other Gradient Boosting implementation (http://datascience.la/benchmarking-random-forest-implementations/)\n",
    "- C++ library\n",
    "\n",
    "### XGBoost supports the following main interfaces:\n",
    "- Command Line Interface (CLI).\n",
    "- C++ (the language in which the library is written).\n",
    "- Python interface as well as a model in scikit-learn.\n",
    "- R interface as well as a model in the caret package.\n",
    "- Julia support.\n",
    "- Java and JVM languages like Scala and platforms like Hadoop.\n",
    "\n",
    "### Model Features\n",
    "- gradient boosting machine including the learning rate\n",
    "- Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels\n",
    "- uses the regularization\n",
    "\n",
    "#### When to use XGBoost?\n",
    "- When there is a larger number of training samples. Ideally, greater than 1000 training samples and less 100 features or we can say when the number of features < number of training samples.\n",
    "- When there is a mixture of categorical and numeric features or just numeric features.\n",
    "\n",
    "#### Main idea \n",
    "- minimize the objective function\n",
    "- apply Taylorâ€™s Theorem so can use objective (loss) function as a simple function of the new added learner \n",
    "- build a learner that achieves the maximum possible reduction of loss, \n",
    "- we can't enumerate all the possible tree structures so we must use \"Exact Greedy Algorithm\"\n",
    "\n",
    "#### How to build the new learner\n",
    "\n",
    "- Start with single root (contains all the training examples)\n",
    "- Iterate over all features and values per feature, and evaluate each possible split loss reduction:\n",
    "- gain = loss(father instances) - (loss(left branch)+loss(right branch))\n",
    "- The gain for the best split must be positive (and > min_split_gain parameter), otherwise we must stop growing the branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example  1a (Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T00:22:49.492674Z",
     "start_time": "2026-01-29T00:22:49.428906800Z"
    }
   },
   "outputs": [],
   "source": [
    "#survivors on Titanic\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "train_data = pd.read_csv(\".\\\\titanic\\\\train.csv\")\n",
    "test_data = pd.read_csv(\".\\\\titanic\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T00:22:49.571206200Z",
     "start_time": "2026-01-29T00:22:49.498824500Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24948\\1163053956.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Survived\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Survived\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#prepare data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mfull_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#remove unnecessary columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdrop_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"Name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Age\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"SibSp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Ticket\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cabin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Parch\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Embarked\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# create output vector\n",
    "y_train = train_data[\"Survived\"]\n",
    "train_data.drop(labels=\"Survived\", axis=1, inplace=True)\n",
    "\n",
    "#prepare data\n",
    "full_data = train_data.append(test_data)\n",
    "\n",
    "#remove unnecessary columns\n",
    "drop_columns = [\"Name\", \"Age\", \"SibSp\", \"Ticket\", \"Cabin\", \"Parch\", \"Embarked\"]\n",
    "full_data.drop(labels=drop_columns, axis=1, inplace=True)\n",
    "\n",
    "full_data = pd.get_dummies(full_data, columns=[\"Sex\"])\n",
    "full_data.fillna(value=0.0, inplace=True)\n",
    "\n",
    "X_train = full_data.values[0:891]\n",
    "X_test = full_data.values[891:]\n",
    "\n",
    "# scale date\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Divide into training and validation data\n",
    "\n",
    "state = 12  \n",
    "test_size = 0.30  \n",
    "  \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Gradient Boosting classifier - learning rate optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-29T00:22:49.539710800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learning_rate \u001b[38;5;129;01min\u001b[39;00m lr_list:\n\u001b[0;32m      4\u001b[0m     gb_clf \u001b[38;5;241m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     gb_clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate: \u001b[39m\u001b[38;5;124m\"\u001b[39m, learning_rate)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy score (training): \u001b[39m\u001b[38;5;132;01m{0:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(gb_clf\u001b[38;5;241m.\u001b[39mscore(X_train, y_train)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n",
    "    gb_clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_val, y_val)))\n",
    "\n",
    "#Based on the data above is higher learning rate always better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-29T00:22:49.733931700Z"
    }
   },
   "outputs": [],
   "source": [
    "#use GRB model where learning_rate=0,5 to generate predictions \n",
    "gb_clf2 = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5, max_features=2, max_depth=2, random_state=0)\n",
    "gb_clf2.fit(X_train, y_train)\n",
    "predictions = gb_clf2.predict(X_val)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, predictions))\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_val, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example 1b (XGBoost) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-29T00:22:49.735721400Z"
    }
   },
   "outputs": [],
   "source": [
    "#survivors on Titanic\n",
    "\n",
    "#!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "score = xgb_clf.score(X_val, y_val)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-29T00:22:49.741940600Z",
     "start_time": "2026-01-29T00:22:49.737260300Z"
    }
   },
   "outputs": [],
   "source": [
    "# k-fold cross validation evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# load data\n",
    "dataset = loadtxt('diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# CV model\n",
    "model = XGBClassifier()\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-29T00:22:49.738798700Z"
    }
   },
   "outputs": [],
   "source": [
    "# stratified k-fold cross validation evaluation of xgboost model\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# load data\n",
    "dataset = loadtxt('diabetes.csv', delimiter=\",\")\n",
    "# split data into X and y\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# CV model\n",
    "model = XGBClassifier()\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-29T00:22:49.740333700Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
