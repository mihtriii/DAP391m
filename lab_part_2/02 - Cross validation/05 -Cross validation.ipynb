{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias - Variance dillema - why we use validation?\n",
    "How can we be sure that our algorithm is really representing the round truth of $F(X)$ and is not meorizing the dataset $y$? We now that it is impossible to *truly* know the $F(X)$. In most cases we will also not memorize the dataset. Created estimator $\\hat{F(X))}$ will be something inbetween.\n",
    "\n",
    "<img src=\"img/biasVarianceCurve.jpg\" width=\"70%\">\n",
    "\n",
    "Source: https://www.quora.com/What-is-the-best-way-to-explain-the-bias-variance-trade-off-in-layman%E2%80%99s-terms\n",
    "\n",
    "This problem is called Bias-Variance tradeoff. The theory behind it is well established and described with mathematical precision in the literature. Here we will focus on the intuition.\n",
    "\n",
    "The more freedom we give to our estimator (more elasticity/variance) the easier/better it will fit the training data. Therefore it will achieve lower error (bias) on the training set. At the same time the higher the variability the higher the risk of overfitting. Although theoretically we can be lowering the expected value of error (bias) it is usually done with the increase of error variance. We can land in a situation where, when we make prediction on new data, the error is very high due to variance (even though it can be zero on average).\n",
    "\n",
    "To understand it lets look at this picture:\n",
    "<img src=\"img/biasVarianceTarget.jpg\" width=\"30%\">\n",
    "\n",
    "Source: https://www.quora.com/What-is-the-best-way-to-explain-the-bias-variance-trade-off-in-layman%E2%80%99s-terms\n",
    "\n",
    "In practice higher variance usually means higher error (on new data). This problem is perfectly visible in the following illustration.\n",
    "<img src=\"img/biasVarianceValid.png\" width=\"50%\">\n",
    "Source: Elements of Statistical learning\n",
    "\n",
    "The red line represents the error on \"new\" data and blue line on the training data. When we look at this image it seems that the answer to the question of optimal complexity. In the end we want our model to work as good as possible on new data and not on training data. It seems that chosing the complexity level for which the red line is the lowest is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation (CV)\n",
    "Cross validation is in the esence a process as described above. We split the data into training and validation samples. We use training data to create our estimator and validation data to check how it will work on \"new\", out of sample data.\n",
    "<img src=\"img/cv.jpg\">\n",
    "Source: Wikipedia\n",
    "\n",
    "In such process a lot depends on the realization of the random split. The results may vary heavily on different realizations. We cannot treat the value in a single validation as a very good approximation of ground truth.\n",
    "\n",
    "### Cross Validation - what for?\n",
    "We use cross validation mainly  for one of two reasins:\n",
    "* Getting an objective evaluation of \"real\" effectivness of our model.\n",
    "* Tuning of models hyperparameters. Hyperaparameters are those arguments that we need to send by hand, are not part of algorithm optimization. Model complexity is often on of those hyper parameters as in the example above.\n",
    "\n",
    "### K-Fold Cross validation (k-fold CV)\n",
    "To get more reliable evaluation of our model a k-fold cross validation is used. We repeat the process of CV K times in such a way that 1/K of all $y$ observations are used for validation and the rest for training. We repeat the procedure K times in such a way that every observation is used for validation only once.\n",
    "<img src=\"img/cvKfold.jpg\">\n",
    "Source: Wikipedia\n",
    "\n",
    "Most common values for K are 5 or 10. In the end a result of k-fold cv is the average value of our metric across all validation sets.\n",
    "\n",
    "*Notice.* The result of k-fold cv will be different each time as the process of splitting is random.\n",
    "\n",
    "### Leave-One-Out Cross Validation (LOO CV)\n",
    "The moste extreme form of k-fold CV is Leave-One-Out Cross Validation (LOO CV). In this case K is equal to N, where N is total number of observations. One might think that this must be \"the best\" CV method as it is the most detailed with a lot of averaging and our training data is as big as possible each time. However, there are shortcomings. Each time our estimator will be very similar as the differences between training data are small. This decreases the value of information we get in the cv process in this case as we do not test our algorithm in \"different scenarios\" (treining sets). One important characteristic of LOO CV is that we get the same result each time we run it.\n",
    "\n",
    "### Repeated Cross Validation\n",
    "In most cases running a k-fold CV is good enough. Sometimes however we need to get as accurate evaluation as possible. We can take advantage of the randomness and simply run k-fold several times and average the results.\n",
    "\n",
    "### Bootstrap cross validation\n",
    "Last cv technique is bootstrap. It works by sampling with replacement N observations from our data set of N observations as training data. As we draw with replacement some observations are repeated and some are not drawn at all. The samples that have not been drawn are treated as validation set. We repeat this procedure multiple times to get the distribution of evaluation metric on validation set.\n",
    "\n",
    "## CV Methods - which one to chose?\n",
    "There is no consensus about the best CV method. Neither among practicioners nor in academia. Theoretically it seems that k-fold (repeated) and bootstrap are two main choices. In everyday use k-fold seems most popular. It may be due to the fact that when we do bootstrap cv there is a good chance that some observations will never be part of validation set. Additionally some people see duplication of observations as an important shortcoming.\n",
    "\n",
    "If we decide to use k-fold there is one more question to answer. Should it be 5, 10 or some other k-fold? On one hand we want our training set to be as big as possible to avoid overfitting. On the other we want to have some variability of training set and stability of results in validation sets. Again, there is no one best answer and one should test different options and do repeated k-fold if more precision is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to setup Cross Validation?\n",
    "Although in theory the procedure of CV is very simple, it is very easy to make mistakes that in some scenarios are very costly.\n",
    "\n",
    "### Stratify\n",
    "One of the problems we can encounter is the balance of labels in training and validation sets. This is especially important when we work on imbalanced data. In datasets where there are few \"1\" in general it is easy to imagine a situation in which we draw no or relatively very little \"1\"s. It can lead to bias and/or big variance of our evaluation metric on the validation data set. Because of this problem we often enforce equal balance using stratification.\n",
    "\n",
    "For example in case of 5-fold cv you can imagine that we make the split for traingin and validation sets separately for \"1\"s and \"0\"s. This quarantees equal balance in both groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series analysis\n",
    "The hardest dataset to set up proper cross validation is time series analysis. Due to the fact that all observations are usualy linked (not independant over time), each observation depends on previous realizations. Because of this property we cannot simply randomly draw observations into two samples and use different permutations. Observations that are next to each other in time contain simply too much information about each other to have them in different sets (train and validation).\n",
    "\n",
    "One solution is to make a natural time-based split. We use \"past\" observations as training data and \"future\" observations as validation set. Just like in the illustration below.\n",
    "<img src=\"img/cvTimeSeries.png\" width=\"50%\">\n",
    "\n",
    "Source: https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information leakage\n",
    "Independance of observation between training and validation set is a crucial requirement for the procedure to deliver expected results. Its importance is clearly visible in time series analysis. However, it is also possible to have information leakage even in regular classification and regression problems, when the independence requirement is not met.\n",
    "\n",
    "In this case, when observations are not independent, we cannot treat validation set as a true *out of sample* data. This results in underestimation of error on validation set and can lead to heavy overestimation and result in a model that simply doesn't work in deployment.\n",
    "\n",
    "Having mutiple observations for one person in our data set is one example of data structure in which it is easy to break independence of data. Other example might be usage of data from many persons from one household.\n",
    "\n",
    "The errors in setup of cross validation process are very easy to make and hard to spot. Even in competition on kaggle data leakage happens from time to time when the sets are prepared by experienced professionals. Always try to know your data and set up your CV with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validate Test\n",
    "At this point we should point to one important problem with the statements made at the begining. We said that we can use results from validation set to choose values for hyperparameters of our algorithm. However, this means that we are transfering information from validation set to training (data leakage). Doing this, if abused, can lead to biased evaluation and suboptimal model. Usually the information leakage is small and insignificant, however in cases where we heavily rely on validation set for algorithm tuning, we should use train, validate, test procedure. The idea is exactly the same as in validation but we add one more step. We save part of the data (test) at the begining that we do not use at all in training process. Then, in the end, when everything is ready we test our model with it.\n",
    "<img src=\"img/cvTVT.png\">\n",
    "Source:https://am207.github.io/2017/wiki/validation.html\n",
    "\n",
    "### Train Validate Test in practice\n",
    "Splitting our dataset in three parts, as everything in machinr learning, has both advantages and shortcomings. Obviously the main advantage is getting pure and reliable evaluation of the model. We cannot forget about disadvantages too:\n",
    "* Separating the test set decreases the number of observations in our training set. It increases the risk of overfitting and cen results in a worse predictive power of our model.\n",
    "* In a basic approach we separate only one part of the set as test. This means that we rely again on one realization of some  random process. This means that we can get very different results reach time even if we repeat whole training procedure exactly the same.\n",
    "\n",
    "Aforementioned problems have two possible solutions. We can do a k-fold train-test/validate. It reduces the variance of the results in test set. On the other hand, it increases the complexity of our ML pipeline and does not alleviate the problem of reduced number of observations for training.\n",
    "\n",
    "Alternatively we can say that our real test set will come later. We will simply wait for new data from the market that will be generated at the time of our model creation. This type of test data is perfect as it comes from the future. This is often a good choice as correctly implemented repeated k-fold cv should bu fully sufficient to chose the best model and optimize its hyperparameters.\n",
    "\n",
    "In the end one needs to chose a proper solution for a specific problem remembering about the type of algorithms we are planning to use and understanding the business need for current ML problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation in Python\n",
    "Most of the methods for cross validation are really well implemented in sklearn.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:43.121407Z",
     "start_time": "2021-03-24T16:21:42.550259Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[32m      9\u001b[39m plt.style.use(\u001b[33m'\u001b[39m\u001b[33mseaborn-ticks\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minline\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "plt.style.use('seaborn-ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get back to our medical set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:45.169405Z",
     "start_time": "2021-03-24T16:21:45.077783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\programy\\Python39\\lib\\site-packages\\scipy\\.libs\n",
      "(35072, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UMARSTAT</th>\n",
       "      <th>UCUREMP</th>\n",
       "      <th>UCURNINS</th>\n",
       "      <th>USATMED</th>\n",
       "      <th>URELATE</th>\n",
       "      <th>REGION</th>\n",
       "      <th>STATE</th>\n",
       "      <th>HHID</th>\n",
       "      <th>FHOSP</th>\n",
       "      <th>FDENT</th>\n",
       "      <th>FEMER</th>\n",
       "      <th>FDOCT</th>\n",
       "      <th>UIMMSTAT</th>\n",
       "      <th>U_USBORN</th>\n",
       "      <th>UAGE</th>\n",
       "      <th>U_FTPT</th>\n",
       "      <th>U_WKSLY</th>\n",
       "      <th>U_HRSLY</th>\n",
       "      <th>U_USHRS</th>\n",
       "      <th>HEARNVAL</th>\n",
       "      <th>HOTHVAL</th>\n",
       "      <th>HRETVAL</th>\n",
       "      <th>HSSVAL</th>\n",
       "      <th>HWSVAL</th>\n",
       "      <th>UBRACE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>UEDUC3</th>\n",
       "      <th>CEYES</th>\n",
       "      <th>CHAIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Never married</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>2</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>55616128</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>22</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>hazel</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Separated</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>2</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54704000</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>30</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>31468</td>\n",
       "      <td>5950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31468</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>HS diploma or GED, no bachelor's degree</td>\n",
       "      <td>blue</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Married_live together</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>5</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>57874272</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>33</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>24700</td>\n",
       "      <td>11340</td>\n",
       "      <td>0</td>\n",
       "      <td>4920</td>\n",
       "      <td>24700</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>brown</td>\n",
       "      <td>brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Divorced</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Little dissatisfied</td>\n",
       "      <td>4</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54106816</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>41</td>\n",
       "      <td>Part-time</td>\n",
       "      <td>43.0</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>60000</td>\n",
       "      <td>39002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60000</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>No HS diploma or GED</td>\n",
       "      <td>brown</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Never married</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Very satisfied</td>\n",
       "      <td>0</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>WI</td>\n",
       "      <td>54569152</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>US-born citizen</td>\n",
       "      <td>Yes</td>\n",
       "      <td>34</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>52.0</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>55280</td>\n",
       "      <td>4200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55280</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>HS diploma or GED, no bachelor's degree</td>\n",
       "      <td>brown</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                UMARSTAT UCUREMP UCURNINS              USATMED  URELATE  \\\n",
       "0          Never married      No      Yes       Very satisfied        2   \n",
       "1              Separated     Yes       No       Very satisfied        2   \n",
       "2  Married_live together      No       No       Very satisfied        5   \n",
       "3               Divorced      No      Yes  Little dissatisfied        4   \n",
       "4          Never married     Yes       No       Very satisfied        0   \n",
       "\n",
       "    REGION STATE      HHID FHOSP  FDENT  FEMER  FDOCT         UIMMSTAT  \\\n",
       "0  Midwest    WI  55616128    No      0      0      0  US-born citizen   \n",
       "1  Midwest    WI  54704000    No      2      0      0  US-born citizen   \n",
       "2  Midwest    WI  57874272    No      0      1      0  US-born citizen   \n",
       "3  Midwest    WI  54106816    No      0      0      1  US-born citizen   \n",
       "4  Midwest    WI  54569152    No      2      0      0  US-born citizen   \n",
       "\n",
       "  U_USBORN  UAGE     U_FTPT  U_WKSLY  U_HRSLY  U_USHRS  HEARNVAL  HOTHVAL  \\\n",
       "0      Yes    22  Full-time     52.0       40       40         0        0   \n",
       "1      Yes    30  Full-time     52.0       40       40     31468     5950   \n",
       "2      Yes    33  Part-time     52.0       30       30     24700    11340   \n",
       "3      Yes    41  Part-time     43.0       40       25     60000    39002   \n",
       "4      Yes    34  Full-time     52.0       40       40     55280     4200   \n",
       "\n",
       "   HRETVAL  HSSVAL  HWSVAL UBRACE  GENDER  \\\n",
       "0        0       0       0  White  Female   \n",
       "1        0       0   31468  White  Female   \n",
       "2        0    4920   24700  White    Male   \n",
       "3        0       0   60000  Black  Female   \n",
       "4        0       0   55280  Black    Male   \n",
       "\n",
       "                                    UEDUC3  CEYES  CHAIR  \n",
       "0                     No HS diploma or GED  hazel  brown  \n",
       "1  HS diploma or GED, no bachelor's degree   blue  black  \n",
       "2                     No HS diploma or GED  brown  brown  \n",
       "3                     No HS diploma or GED  brown  black  \n",
       "4  HS diploma or GED, no bachelor's degree  brown  black  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd C:\n",
    "medical = pd.read_csv(\"E:/OneDrive/WNE/SEM2/2400-DS1ML1 Machine Learning 1 classification methods/Class 1/data/medical_care.csv\")\n",
    "print(medical.shape)\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets encode UCURNINS variable and run the regression on our full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:46.204766Z",
     "start_time": "2021-03-24T16:21:46.197124Z"
    }
   },
   "outputs": [],
   "source": [
    "medical[\"UCURNINS\"] = (medical.UCURNINS == \"Yes\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:21:54.887758Z",
     "start_time": "2021-03-24T16:21:54.064105Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>UCURNINS</td>     <th>  No. Observations:  </th>  <td> 35072</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 35036</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>    35</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -11126.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 29 Mar 2022</td> <th>  Deviance:          </th> <td>  22251.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>19:37:05</td>     <th>  Pearson chi2:      </th> <td>4.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>6</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.1742</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                          <td></td>                             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                         <td>    0.9019</td> <td>    0.216</td> <td>    4.166</td> <td> 0.000</td> <td>    0.478</td> <td>    1.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married, do not live together]</th>         <td>   -0.4264</td> <td>    0.166</td> <td>   -2.563</td> <td> 0.010</td> <td>   -0.752</td> <td>   -0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Married_live together]</th>                 <td>   -0.8331</td> <td>    0.056</td> <td>  -14.774</td> <td> 0.000</td> <td>   -0.944</td> <td>   -0.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Never married]</th>                         <td>   -0.3337</td> <td>    0.064</td> <td>   -5.210</td> <td> 0.000</td> <td>   -0.459</td> <td>   -0.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Partnership]</th>                           <td>    0.3966</td> <td>    0.085</td> <td>    4.677</td> <td> 0.000</td> <td>    0.230</td> <td>    0.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Separated]</th>                             <td>   -0.0527</td> <td>    0.095</td> <td>   -0.553</td> <td> 0.580</td> <td>   -0.239</td> <td>    0.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Unknown]</th>                               <td>    0.6720</td> <td>    0.388</td> <td>    1.732</td> <td> 0.083</td> <td>   -0.089</td> <td>    1.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UMARSTAT[T.Widowed]</th>                               <td>   -0.1727</td> <td>    0.144</td> <td>   -1.197</td> <td> 0.231</td> <td>   -0.455</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Little satisfied]</th>                       <td>   -0.4597</td> <td>    0.063</td> <td>   -7.324</td> <td> 0.000</td> <td>   -0.583</td> <td>   -0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.No opinion]</th>                             <td>    0.6913</td> <td>    0.098</td> <td>    7.025</td> <td> 0.000</td> <td>    0.498</td> <td>    0.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Very dissatisfied]</th>                      <td>    0.5547</td> <td>    0.092</td> <td>    6.017</td> <td> 0.000</td> <td>    0.374</td> <td>    0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>USATMED[T.Very satisfied]</th>                         <td>   -0.7287</td> <td>    0.062</td> <td>  -11.668</td> <td> 0.000</td> <td>   -0.851</td> <td>   -0.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.Northeast]</th>                               <td>   -0.0486</td> <td>    0.057</td> <td>   -0.858</td> <td> 0.391</td> <td>   -0.159</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.South]</th>                                   <td>    0.7221</td> <td>    0.047</td> <td>   15.439</td> <td> 0.000</td> <td>    0.630</td> <td>    0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REGION[T.West]</th>                                    <td>    0.4020</td> <td>    0.049</td> <td>    8.143</td> <td> 0.000</td> <td>    0.305</td> <td>    0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FHOSP[T.Yes]</th>                                      <td>   -0.2438</td> <td>    0.075</td> <td>   -3.254</td> <td> 0.001</td> <td>   -0.391</td> <td>   -0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.Foreign-born, non-citizen]</th>             <td>    0.7271</td> <td>    0.089</td> <td>    8.172</td> <td> 0.000</td> <td>    0.553</td> <td>    0.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UIMMSTAT[T.US-born citizen]</th>                       <td>   -0.5706</td> <td>    0.080</td> <td>   -7.114</td> <td> 0.000</td> <td>   -0.728</td> <td>   -0.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_FTPT[T.Part-time]</th>                               <td>    0.4966</td> <td>    0.058</td> <td>    8.526</td> <td> 0.000</td> <td>    0.382</td> <td>    0.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Asian/Pacific Islander]</th>                  <td>   -0.9828</td> <td>    0.161</td> <td>   -6.104</td> <td> 0.000</td> <td>   -1.298</td> <td>   -0.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.Black]</th>                                   <td>   -0.4685</td> <td>    0.126</td> <td>   -3.710</td> <td> 0.000</td> <td>   -0.716</td> <td>   -0.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UBRACE[T.White]</th>                                   <td>   -0.5119</td> <td>    0.118</td> <td>   -4.338</td> <td> 0.000</td> <td>   -0.743</td> <td>   -0.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UEDUC3[T.HS diploma or GED, no bachelor's degree]</th> <td>    0.8375</td> <td>    0.053</td> <td>   15.828</td> <td> 0.000</td> <td>    0.734</td> <td>    0.941</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UEDUC3[T.No HS diploma or GED]</th>                    <td>    1.5713</td> <td>    0.063</td> <td>   24.881</td> <td> 0.000</td> <td>    1.448</td> <td>    1.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GENDER[T.Male]</th>                                    <td>    0.1413</td> <td>    0.039</td> <td>    3.652</td> <td> 0.000</td> <td>    0.065</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>URELATE</th>                                           <td>    0.0161</td> <td>    0.012</td> <td>    1.350</td> <td> 0.177</td> <td>   -0.007</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDENT</th>                                             <td>   -0.3159</td> <td>    0.015</td> <td>  -21.188</td> <td> 0.000</td> <td>   -0.345</td> <td>   -0.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FEMER</th>                                             <td>    0.0871</td> <td>    0.022</td> <td>    3.976</td> <td> 0.000</td> <td>    0.044</td> <td>    0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FDOCT</th>                                             <td>   -0.1353</td> <td>    0.008</td> <td>  -16.404</td> <td> 0.000</td> <td>   -0.151</td> <td>   -0.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UAGE</th>                                              <td>   -0.0182</td> <td>    0.002</td> <td>   -9.766</td> <td> 0.000</td> <td>   -0.022</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_WKSLY</th>                                           <td>   -0.0197</td> <td>    0.002</td> <td>  -12.732</td> <td> 0.000</td> <td>   -0.023</td> <td>   -0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>U_USHRS</th>                                           <td>    0.0022</td> <td>    0.002</td> <td>    1.234</td> <td> 0.217</td> <td>   -0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HOTHVAL</th>                                           <td>-1.179e-07</td> <td> 1.57e-06</td> <td>   -0.075</td> <td> 0.940</td> <td>-3.19e-06</td> <td> 2.95e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HRETVAL</th>                                           <td> 5.256e-06</td> <td> 3.18e-06</td> <td>    1.650</td> <td> 0.099</td> <td>-9.86e-07</td> <td> 1.15e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HSSVAL</th>                                            <td>-5.631e-06</td> <td> 3.77e-06</td> <td>   -1.494</td> <td> 0.135</td> <td> -1.3e-05</td> <td> 1.76e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HWSVAL</th>                                            <td>  9.47e-09</td> <td> 3.24e-07</td> <td>    0.029</td> <td> 0.977</td> <td>-6.26e-07</td> <td> 6.45e-07</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:               UCURNINS   No. Observations:                35072\n",
       "Model:                            GLM   Df Residuals:                    35036\n",
       "Model Family:                Binomial   Df Model:                           35\n",
       "Link Function:                  Logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -11126.\n",
       "Date:                Tue, 29 Mar 2022   Deviance:                       22251.\n",
       "Time:                        19:37:05   Pearson chi2:                 4.05e+04\n",
       "No. Iterations:                     6   Pseudo R-squ. (CS):             0.1742\n",
       "Covariance Type:            nonrobust                                         \n",
       "=====================================================================================================================\n",
       "                                                        coef    std err          z      P>|z|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                             0.9019      0.216      4.166      0.000       0.478       1.326\n",
       "UMARSTAT[T.Married, do not live together]            -0.4264      0.166     -2.563      0.010      -0.752      -0.100\n",
       "UMARSTAT[T.Married_live together]                    -0.8331      0.056    -14.774      0.000      -0.944      -0.723\n",
       "UMARSTAT[T.Never married]                            -0.3337      0.064     -5.210      0.000      -0.459      -0.208\n",
       "UMARSTAT[T.Partnership]                               0.3966      0.085      4.677      0.000       0.230       0.563\n",
       "UMARSTAT[T.Separated]                                -0.0527      0.095     -0.553      0.580      -0.239       0.134\n",
       "UMARSTAT[T.Unknown]                                   0.6720      0.388      1.732      0.083      -0.089       1.433\n",
       "UMARSTAT[T.Widowed]                                  -0.1727      0.144     -1.197      0.231      -0.455       0.110\n",
       "USATMED[T.Little satisfied]                          -0.4597      0.063     -7.324      0.000      -0.583      -0.337\n",
       "USATMED[T.No opinion]                                 0.6913      0.098      7.025      0.000       0.498       0.884\n",
       "USATMED[T.Very dissatisfied]                          0.5547      0.092      6.017      0.000       0.374       0.735\n",
       "USATMED[T.Very satisfied]                            -0.7287      0.062    -11.668      0.000      -0.851      -0.606\n",
       "REGION[T.Northeast]                                  -0.0486      0.057     -0.858      0.391      -0.159       0.062\n",
       "REGION[T.South]                                       0.7221      0.047     15.439      0.000       0.630       0.814\n",
       "REGION[T.West]                                        0.4020      0.049      8.143      0.000       0.305       0.499\n",
       "FHOSP[T.Yes]                                         -0.2438      0.075     -3.254      0.001      -0.391      -0.097\n",
       "UIMMSTAT[T.Foreign-born, non-citizen]                 0.7271      0.089      8.172      0.000       0.553       0.901\n",
       "UIMMSTAT[T.US-born citizen]                          -0.5706      0.080     -7.114      0.000      -0.728      -0.413\n",
       "U_FTPT[T.Part-time]                                   0.4966      0.058      8.526      0.000       0.382       0.611\n",
       "UBRACE[T.Asian/Pacific Islander]                     -0.9828      0.161     -6.104      0.000      -1.298      -0.667\n",
       "UBRACE[T.Black]                                      -0.4685      0.126     -3.710      0.000      -0.716      -0.221\n",
       "UBRACE[T.White]                                      -0.5119      0.118     -4.338      0.000      -0.743      -0.281\n",
       "UEDUC3[T.HS diploma or GED, no bachelor's degree]     0.8375      0.053     15.828      0.000       0.734       0.941\n",
       "UEDUC3[T.No HS diploma or GED]                        1.5713      0.063     24.881      0.000       1.448       1.695\n",
       "GENDER[T.Male]                                        0.1413      0.039      3.652      0.000       0.065       0.217\n",
       "URELATE                                               0.0161      0.012      1.350      0.177      -0.007       0.039\n",
       "FDENT                                                -0.3159      0.015    -21.188      0.000      -0.345      -0.287\n",
       "FEMER                                                 0.0871      0.022      3.976      0.000       0.044       0.130\n",
       "FDOCT                                                -0.1353      0.008    -16.404      0.000      -0.151      -0.119\n",
       "UAGE                                                 -0.0182      0.002     -9.766      0.000      -0.022      -0.015\n",
       "U_WKSLY                                              -0.0197      0.002    -12.732      0.000      -0.023      -0.017\n",
       "U_USHRS                                               0.0022      0.002      1.234      0.217      -0.001       0.006\n",
       "HOTHVAL                                           -1.179e-07   1.57e-06     -0.075      0.940   -3.19e-06    2.95e-06\n",
       "HRETVAL                                            5.256e-06   3.18e-06      1.650      0.099   -9.86e-07    1.15e-05\n",
       "HSSVAL                                            -5.631e-06   3.77e-06     -1.494      0.135    -1.3e-05    1.76e-06\n",
       "HWSVAL                                              9.47e-09   3.24e-07      0.029      0.977   -6.26e-07    6.45e-07\n",
       "=====================================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = medical,\n",
    "                          family = sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5680/952132274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple splits we can use train_test_split. This method will split the data set of features and a column with our dependant variable.\n",
    "We can define the random state or initialize it with a random number. Having control over randomness of our splits can help a lot with reproductibility in some cases. To see the differenr run a couple times a cell above and below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:22:55.211066Z",
     "start_time": "2021-03-24T16:22:54.508044Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(medical,\n",
    "                                                    medical.UCURNINS,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = random.randint(0, 1000))\n",
    "print(X_train.shape, X_test.shape)\n",
    "mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = X_train,\n",
    "                          family = sm.families.Binomial())\n",
    "res = mod.fit()\n",
    "res.summary()\n",
    "preds = res.predict(X_test)\n",
    "\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can very easily emply stratify with just one argument. We just need to set the column for stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:26:27.953257Z",
     "start_time": "2021-03-24T16:25:50.835743Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for k in range(100) :\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(medical,\n",
    "                                                        medical.UCURNINS,\n",
    "                                                        stratify = medical.UCURNINS,\n",
    "                                                        test_size = 0.3,\n",
    "                                                        random_state = random.randint(0, 10000))\n",
    "    # print(X_train.shape, X_test.shape)\n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = X_train,\n",
    "                              family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    res.summary()\n",
    "    preds = res.predict(X_test)xá»¯\n",
    "    scores.append(roc_auc_score(y_test, preds))\n",
    "    # print(scores[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in case of logistic regression on this data set there is very little difference with and without stratification. Our \"1\" label is represented well enough (5 000 observations) for randomness to work well enough with 0.7 and 0.3 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical.UCURNINS.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see some diagnostics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = scores, columns = ['scores'])\n",
    "print(df.scores.describe())\n",
    "print(f'Kurtosis \\t %s' %round(df.scores.kurtosis(), 5)) # This one returns 'normalized' kurtosis (i.e. 0 instead of 3).\n",
    "print(f'Skewness \\t %s' %round(df.scores.skew(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.displot(data = df.scores, kde = True, label = 'empirical hist')\n",
    "x0, x1 = ax.ax.get_xlim()\n",
    "x_pdf = np.linspace(x0, x1, len(df))\n",
    "y_pdf = stats.norm.pdf(x_pdf, df.scores.mean(), df.scores.std())\n",
    "ax.ax.plot(x_pdf, y_pdf, 'r', lw = 2, label = 'normal pdf')\n",
    "ax.ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "\n",
    "alpha = 1e-3\n",
    "\n",
    "k2, p = stats.normaltest(df.scores) # Kurtosis and skewness normality test.\n",
    "\n",
    "if p < alpha :\n",
    "\n",
    "    print('Kurtosis, Skewness test: The null hypothesis about normality can be rejected.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Kurtosis, Skewness test: The null hypothesis about normality can not be rejected.')\n",
    "\n",
    "# Kolmogorov Smirnov normality test.\n",
    "ks = stats.kstest(df.scores, 'norm')\n",
    "\n",
    "if ks[1] < alpha :\n",
    "\n",
    "    print('Kolmogorov Smirnov test: The null hypothesis about normality can be rejected.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Kolmogorov Smirnov test: The null hypothesis about normality can not be rejected.')\n",
    "\n",
    "# Do you remember that @kstest function requires NORMALIZATION of sample?\n",
    "ks = stats.kstest((df.scores - df.scores.mean()) / df.scores.std(), 'norm')\n",
    "\n",
    "if ks[1] < alpha :\n",
    "\n",
    "    print('Kolmogorov Smirnov (normalized sample) test: The null hypothesis about normality can be rejected.')\n",
    "\n",
    "else:\n",
    "\n",
    "    print('Kolmogorov Smirnov (normalized sample) test: The null hypothesis about normality can not be rejected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the tendency to overfitting in logistic regression. As you can see below we need to reduct the train data set size to only 10% to really start overfitting. This show great resilience to overfitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:27:56.503508Z",
     "start_time": "2021-03-24T16:27:50.716171Z"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(1, 10) :\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(medical,\n",
    "                                                        medical.UCURNINS,\n",
    "                                                        test_size = 0.1 * k,\n",
    "                                                        random_state = 0)\n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = X_train,\n",
    "                              family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(X_train)\n",
    "    preds = res.predict(X_test)\n",
    "    print(\"Train AUC:\", round(roc_auc_score(y_train, predsTrain), 4), \"Valid AUC:\", round(roc_auc_score(y_test, preds), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For k-fold CV we do not get datasets but rather indices for the sets. This is done to avoid unecessary duplication of datasets. Lets run 10-fold CV and see the results. For k-fold we need to add shuffle = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:29:09.719179Z",
     "start_time": "2021-03-24T16:29:01.573294Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    print(\"Train AUC:\", round(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain), 4), \"Valid AUC:\",\n",
    "          round(roc_auc_score(medical.iloc[test].UCURNINS, preds), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As we can see the results vary a lot from sample to sample. W see values from 0.81 to 0.838 on validation sets.\n",
    "\n",
    "To understand the problem of variance even more, lets see the results for 5-fold CV. The variance of results seems slightly lower for 10-fold CV. This is natural as logistic regression does not overfit too much and we increase the size of validation set (that stabilizes the results on validation samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:31:04.729383Z",
     "start_time": "2021-03-24T16:31:00.883441Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial())    \n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    print(\"Train AUC:\", round(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain), 4),\n",
    "          \"Valid AUC:\", round(roc_auc_score(medical.iloc[test].UCURNINS, preds), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the problem of variance with more detail. Lets run 10-fold CV 10 times and see the variance of averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:33:31.620760Z",
     "start_time": "2021-03-24T16:32:10.356653Z"
    }
   },
   "outputs": [],
   "source": [
    "for z in range(10) :\n",
    "    \n",
    "    trainRes = []\n",
    "    valRes = []\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = random.randint(0, 10000))\n",
    "    \n",
    "    for train, test in kf.split(medical.index.values):\n",
    "        \n",
    "        mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial())          \n",
    "        res = mod.fit()\n",
    "        predsTrain = res.predict(medical.iloc[train])\n",
    "        preds = res.predict(medical.iloc[test])\n",
    "        trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "        valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "        \n",
    "    print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for 5-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:34:14.913581Z",
     "start_time": "2021-03-24T16:33:35.900569Z"
    }
   },
   "outputs": [],
   "source": [
    "for z in range(10) :\n",
    "    \n",
    "    trainRes = []\n",
    "    valRes = []\n",
    "    kf = KFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "    \n",
    "    for train, test in kf.split(medical.index.values) :\n",
    "        \n",
    "        mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                              'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                              'UEDUC3 + GENDER',\n",
    "                              data = medical.iloc[train],\n",
    "                              family = sm.families.Binomial()) \n",
    "        res = mod.fit()\n",
    "        predsTrain = res.predict(medical.iloc[train])\n",
    "        preds = res.predict(medical.iloc[test])\n",
    "        trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "        valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "        \n",
    "    print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that for logistic regression has very stable results for both 5 and 10 fold validations. What's more, it does not overfit too much, as the difference in the value of our metric for train and validation sets is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:37:31.052685Z",
     "start_time": "2021-03-24T16:37:27.263403Z"
    }
   },
   "outputs": [],
   "source": [
    "predList = []\n",
    "indList = []\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = medical.iloc[train],\n",
    "                          family = sm.families.Binomial()) \n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    \n",
    "    predList.append(preds.tolist())\n",
    "    indList.append(medical.iloc[test].index.tolist())\n",
    "    \n",
    "    trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "    valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "    \n",
    "print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:39:05.139258Z",
     "start_time": "2021-03-24T16:39:05.111405Z"
    }
   },
   "outputs": [],
   "source": [
    "predsSorted = pd.Series(sum(predList, []), index = sum(indList, [])).sort_index()\n",
    "roc_auc_score(medical.UCURNINS.sort_index(), pd.Series(sum(predList, []), index = sum(indList, [])).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:40:29.997849Z",
     "start_time": "2021-03-24T16:40:26.239403Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "predList = []\n",
    "indList = []\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = random.randint(0, 10000))\n",
    "\n",
    "for train, test in kf.split(medical.index.values, medical.UCURNINS) :\n",
    "    \n",
    "    mod = sm.GLM.from_formula(formula = 'UCURNINS ~ UMARSTAT + USATMED + URELATE + REGION + FHOSP + FDENT + FEMER + FDOCT + ' + \n",
    "                          'UIMMSTAT + UAGE + U_FTPT + U_WKSLY + U_USHRS + HOTHVAL + HRETVAL + HSSVAL + HWSVAL + UBRACE + ' + \n",
    "                          'UEDUC3 + GENDER',\n",
    "                          data = medical.iloc[train],\n",
    "                          family = sm.families.Binomial())\n",
    "    res = mod.fit()\n",
    "    predsTrain = res.predict(medical.iloc[train])\n",
    "    preds = res.predict(medical.iloc[test])\n",
    "    \n",
    "    predList.append(preds.tolist())\n",
    "    indList.append(medical.iloc[test].index.tolist())\n",
    "    \n",
    "    trainRes.append(roc_auc_score(medical.iloc[train].UCURNINS, predsTrain))\n",
    "    valRes.append(roc_auc_score(medical.iloc[test].UCURNINS, preds))\n",
    "    \n",
    "print(\"Train AUC:\", round(np.mean(trainRes), 4), \"Valid AUC:\", round(np.mean(valRes), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But, let's not forget about overall model quality...\n",
    "Overwhelming majority of folks is insured, just by blind betting on someone's being insured we are correct in 85% of cases, and overall quality of our model is just slighty above it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Overall percent of uninsured persons is %s' %round(medical.UCURNINS.sum() / medical.UCURNINS.count() * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Quality score for training set is %s' \n",
    "      %round(sum(medical.UCURNINS.iloc[train] == (predsTrain > 0.5) * 1) / len(train) * 100, 2))\n",
    "print(f'Quality score for test set is %s' \n",
    "      %round(sum(medical.UCURNINS.iloc[test] == (preds > 0.5) * 1) / len(test) * 100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    #--------------------------------------------------------------------\n",
    "    # Exercises 4.\n",
    "\n",
    "    # Exercise 4.1.\n",
    "\n",
    "    # Titanic passengers data â 1310 observations and 15 variables:\n",
    "    # passenger_id â Unique passenger id\n",
    "    # pclass â Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "    # survived â Survival (0 = No, 1 = Yes)\n",
    "    # name â Name and SUrname\n",
    "    # sex â Sex (0 = Male, 1 = Female)\n",
    "    # age â Age in years\n",
    "    # sibsp â # of siblings / spouses aboard the Titanic\n",
    "    # parch â # of parents / children aboard the Titanic\n",
    "    # ticket â Ticket number\n",
    "    # fare â Passenger fare\n",
    "    # cabin â Cabin number\n",
    "    # embarked â Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "    # boat â Lifeboat (if survived)\n",
    "    # body â Body number (if did not survive and body was recovered)\n",
    "    # home.dest â Home/Destination\n",
    "    # \n",
    "    # Re-run your best models for all algorithms for 5-fold CV. \n",
    "    # Check the stability of results for repeated K-fold\n",
    "    # Check in repeated k-fold CV if adding stratification changes your results (stability)\n",
    "    # Check if you didnt overfit in your models. Check if you can imrpove you validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "plt.style.use('seaborn-v0_8-ticks')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4.2.\n",
    "# Wine Quality Data Set: \"data/wines.csv\"\n",
    "# source: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "# The file contains data on samples of white and red Portuguese wine \n",
    "# Vinho Verde. \n",
    "# Various physico-chemical characteristics of individual samples\n",
    "# are available as well as wine quality scores on a point scale (0-10) \n",
    "# made by specialists.\n",
    "\n",
    "# Re-run your best models for all algorithms for 5-fold CV. \n",
    "# Check the stability of results for repeated K-fold\n",
    "# Check in repeated k-fold CV if adding stratification changes your results (stability).\n",
    "# Compare the effect of stratification with titanic problem.\n",
    "# Check if you didnt overfit in your models. Check if you can imrpove you validation score.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dap391m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
